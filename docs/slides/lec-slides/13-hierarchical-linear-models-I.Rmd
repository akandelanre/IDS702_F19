---
title: "Multilevel/hierarchical linear models I"
author: "Dr. Olanrewaju Michael Akande"
date: "Oct 15, 2019"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
#library(tidyverse)
#library(magick)
library(knitr)
library(kableExtra)
library(lattice)
#library(dplyr)
library(MASS)
library(arm)
library(e1071)
library(caret)
library(pROC)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  #show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 2.65,dpi =300,fig.align='center',fig.show='hold',size='footnotesize',small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}

knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar =  c(4, 4, 1.5, 1.5)) 
})
```

## Announcements
  
- Lab for this week, that is on Fri, Oct 18, will hold in Rm103 and not Rm270!

## Outline

- Questions from last lecture

- Introduction to hierarchical models

- Random effects models

- Mixed effects  models

- In-class analysis


---
class: center, middle

# Hierarchical models


---
## Clustered or grouped data

- Often data are grouped or clustered, for example 
  + students within schools,
  + patients within hospitals,
  + voters within counties or states, or
  + repeated measurements on same person, as is often the case in longitudinal studies.
  
- For such clustered data, we may want to infer or estimate the relationship between a response variable and certain predictors collected across all the groups.

- Ideally, we should do so in a way that takes advantage of the relationship between observations in the same group, but we should also look to borrow information across groups.

- .hlight[Hierarchical or multilevel models] provide a principled way to do so. We'll start with simpler cases to elucidate the main ideas.


---
## Hypothetical school testing example

- Suppose we wish to estimate the distribution of test scores for students at $J$ different high schools.

- In each school $j$, where $j = 1, \ldots, J$, suppose we test a random sample of $n_j$ students.

- Let $y_{ij}$ be the test score for the $i$th student in school $j$, with $i = 1,\ldots, n_j$.

- .hlight[Option I]: estimation can be done separately in each group, where we assume
.block[
.small[
$$y_{ij} | \mu_j, \sigma^2_j \sim N \left( \mu_j, \sigma^2_j\right)$$
]
]

  where for each school $j$, let $\mu_j$ be the school-wide average test score, and $\sigma^2_j$ be the school-wide variance of individual test scores.


---
## Hypothetical school testing example

- We can then do classical inference for each school based on large sample 95% CI: $\bar{y}_j \pm 1.96 \sqrt{s^2_j/n_j}$, where $\bar{y}_j$ is the sample average in school $j$, and $s^2_j$ is the sample variance in school $j$.

- Clearly, we can overfit the data within schools, for example, what if we only have 4 students from one of the schools?

- .hlight[Option II]: alternatively, we might believe that $\mu_j = \mu$ for all $j$; that is, all schools have the same mean. This is the assumption (null hypothesis) in ANOVA models for example.

- Option I ignores that the $\mu_j$'s should be reasonably similar, whereas option II ignores any differences between them.

- It would be nice to find a compromise! This is what we are able to do with .hlight[hierarchical modeling].


---
## Hierarchical model

- Instead, we can assume that the $\mu_j$'s are drawn from a distribution based on the following: conceive of the schools themselves as being a random sample from all possible school.

- Suppose $\mu_0$ is the overall mean of all school's average scores (a mean of the means), and $\tau^2$ is the variance of all school's average scores (a variance of the means).

- Then, we can think of each $\mu_j$ as being drawn from a distribution, for example,
.block[
.small[
$$\mu_j | \mu_0, \tau^2 \sim N \left( \mu_0, \tau^2 \right),$$
]
]

  which gives us one more level, resulting in a heirarchical specification.

- We have to estimate $\sigma^2_j$, $\mu_0$, and $\tau^2$, but let's pretend those are fixed for now (use maximum likelihood or Bayesian methods from STA 601/602).

- In this course, we will only scratch the surface of hierarchical modeling. If you want to explore hierarchical models in much more detail, consider taking STA 610.



---
## Hierarchical model: school testing example

- Back to our example, it turns out that:
.block[
.small[
$$\hat{\mu}_j \approx \dfrac{ \dfrac{n_j}{\sigma^2_j} \bar{y}_j + \dfrac{1}{\tau^2} \mu_0 }{ \dfrac{n_j}{\sigma^2_j} + \dfrac{1}{\tau^2}  }.$$
]
]

- .hlight[For those interested in Bayesian inference] (feel free to skip this if you are not!), it turns out that the posterior distribution of $\mu_j$, $p(\mu_j | Y, \sigma^2_j, \mu_0, \tau^2) = N(\mu_j^\star, \nu_j^\star)$, where 
.block[
.small[
$$
\begin{split}
\mu_j^\star & = \dfrac{ \dfrac{n_j}{\sigma^2_j} \bar{y}_j + \dfrac{1}{\tau^2} \mu_0 }{ \dfrac{n_j}{\sigma^2_j} + \dfrac{1}{\tau^2}  } \\
\nu_j^\star & = \dfrac{1}{ \dfrac{n_j}{\sigma^2_j} + \dfrac{1}{\tau^2}  }
\end{split}
$$
]
]



---
## Hierarchical model: implications

- Our estimate for each $\mu_j$ is a weighted average of $\bar{y}_j$ and $\mu_0$, ensuring that we are borrowing information across all levels through $\mu_0$ and $\tau^2$.

- The weights for the weighted average is determined by relative precisions (the inverse of variance is often referred to as precision) from the data and from the second level model.
  
- Suppose all $\sigma^2_j \approx \sigma^2$. Then the schools with smaller $n_j$ have estimated $\mu_j$ closer to $\mu_0$ than schools with larger $n_j$.

- Thus, the hierarchical model shrinks estimates with high variance towards the grand mean.



---
## Mercury in largemouth bass

- High levels of mercury in fish is known to cause health problems, especially in children. 

- We will use data from the Nicholas School of the Environment's study of mercury concentrations in largemouth bass from the Waccamaw and Lumber rivers in NC. A few details about the data:

  + Mercury cannot be excreted and accumulates in tissue over the lifetime of a fish
  + Direct measurement of mercury requires sacrificing fish
  + Potential predictors include easy-to-measure variables such as weight and/or length
  + Fish were caught, weighed and measured at 16 stations
  + Filets sent to the lab for mercury measurements

  <div class="question">
Can we predict mercury levels for fish caught in these rivers?
</div>







---
class: center, middle

# In-class analysis: move to the R script [here](https://akandelanre.github.io/IDS702_F19/slides/lec-slides/Springbok.R)












