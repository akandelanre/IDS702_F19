<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>MLR: model assessment, diagnostics and validation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MLR: model assessment, diagnostics and validation
### Dr. Olanrewaju Michael Akande
### Sept 3, 2019

---







## Outline

- Recap of last lecture

- Checking model assumptions

- Leverage, influence, and standardized residuals

- Prediction and validation

- Questions


---
## Introduction

- In the last class, we started our introduction to multiple linear regression. 

--

- We saw how to write down and fit MLR models. However, we ignored a very important part of fitting any model: model assessment and validation.

--

- We need to assess whether or not the assumptions of the model actually hold for the data at hand. That's exactly what we will dive in today.

--

- We will also cover model validation via in-sample and out-of-sample predictions.

--

- In the next class, we will move on to transformations (which we will touch on a bit today), multicollinearity and heteroscedasticity.


---
class: center, middle

# Assumptions for MLR


---
## Assumptions for MLR

- Inference (CIs, p-values, or predictions) can only be meaningful when the regression assumptions are plausible.

--

- &lt;div class="question"&gt;
Can you list the assumptions for MLR?
&lt;/div&gt;

--

- The main assumptions are:
  
  1. Linearity
  
  2. Independence of errors
  
  3. Equal variance

  4. Normality


---
## First, think about validity of the model

- Validity is about whether the data and model are even suitable for answering the research question. 

--

- To quote the Gelman and Hill book,

  .block[
  "Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to cases to which it will be applied. 
  
  For example, a model of earnings will not necessarily tell you about pattern of total assets, neither will a model of test scores necessarily tell you about child intelligence or cognitive development."
  ]

--

- You should always keep in mind the types of questions you can and cannot answer reliably from both the data and model.


---
## Checking assumptions

- Checking all four assumptions usually requires examining the residuals after model fitting.

--

- For .hlight[linearity], a plot of the residuals versus each predictor will often do.

--

  &lt;div class="question"&gt;
What should we look out for in such a plot?
&lt;/div&gt;

--

- Note that the residuals contain information about the response variable variable `\(y\)` that has not been explained by the covariates in the model.

--

- Thus, when we plot the residuals against each predictor, we should NOT expect to see any pattern.

--

- .block[
Some pattern in any of the plots is usually an indication of a relationship (often nonlinear) between `\(y\)` and that predictor, which has not been captured yet in the model.
]


---
## Checking assumptions

- To check both .hlight[independence] of the errors and the .hlight[equal variance] assumption, usually a plot of the residuals versus the fitted values will do.

--

- .block[
The points in the plot should look random (for independence) and be "roughly" equally spread out around zero (for equal variance).
]

--

- To check .hlight[normality], it is often sufficient to look at a qq-plot (quantile-quantile plot) which compares the distribution of the standardized residuals to the theoretical quantiles of a standard normal distribution.

--

- .block[
Clustering of the points around the 45 degree line of the qq-plot usually implies the normality assumption is not violated.
]

--

- One can also look at a histogram of the residuals, but it is much harder to judge deviations from normality through histograms.


---
## Checking assumptions

Let's revisit the same data from last class. Recall the model we fit to the data and the results:
.small[
`$$\textrm{bsal}_i = \beta_0 + \beta_1 \textrm{sex}_i + \beta_2 \textrm{senior}_i + \beta_3 \textrm{age}_i + \beta_4 \textrm{educ}_i + \beta_5 \textrm{exper}_i + \epsilon_i$$`
]


```
## 
## Call:
## lm(formula = bsal ~ sex + senior + age + educ + exper, data = wages)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1217.36  -342.83   -55.61   297.10  1575.53 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 6277.8934   652.2713   9.625 2.36e-15 ***
## sexFemale   -767.9127   128.9700  -5.954 5.39e-08 ***
## senior       -22.5823     5.2957  -4.264 5.08e-05 ***
## age            0.6310     0.7207   0.876 0.383692    
## educ          92.3060    24.8635   3.713 0.000361 ***
## exper          0.5006     1.0553   0.474 0.636388    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 508.1 on 87 degrees of freedom
## Multiple R-squared:  0.5152,	Adjusted R-squared:  0.4873 
## F-statistic: 18.49 on 5 and 87 DF,  p-value: 1.811e-12
```

---
## Checking linearity

Now, let's plot the residuals against each predictor. First, let's look at .hlight[senior].

```r
plot(y=regwage$residual, x = wages$senior, xlab = "Seniority", ylab = "Residual")
abline(0,0)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the linearity assumption?
&lt;/div&gt;


---
## Checking linearity

Next, let's look at .hlight[age].

```r
plot(y=regwage$residual, x = wages$age, xlab = "Age", ylab = "Residual")
abline(0,0)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the linearity assumption?
&lt;/div&gt;


---
## Checking linearity

Next, let's look at .hlight[educ].

```r
plot(y=regwage$residual, x = wages$educ, xlab = "Education", ylab = "Residual")
abline(0,0)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the linearity assumption?
&lt;/div&gt;


---
## Checking linearity

Next, let's look at .hlight[exper].

```r
plot(y=regwage$residual, x = wages$exper, xlab = "Experience", ylab = "Residual")
abline(0,0)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the linearity assumption?
&lt;/div&gt;


---
## Checking independence and equal variance


```r
plot(regwage,which=1)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the independence and equal variance assumptions?
&lt;/div&gt;


---
## Checking normality


```r
plot(regwage,which=2)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Do you see any clear violations of the normality assumption?
&lt;/div&gt;


---
## Takeaways from residual plots

- Looks like we may have to worry about the assumption of linearity being violated for age and experience.

--

- There appears to be some quadratic trend for both variables and possible non-constant variance, so let's improve the model by adding the squared term for each variable. 

--

- First, let's mean-center the continuous predictor to improve interpretation of outputs (especially the intercept).

--

- Centering does not really improve model fit, however it does help with interpretability. 

--

- Intercepts especially are often hard to interpret, because they represent value when all predictors equal zero. This may be an unrealistic or uninteresting case.


---
## Centering

- Instead, for each continuous predictor, we can subtract its mean from every value, and use these mean centered predictors in regression instead.

--

- The intercept can now be interpreted as average value of `\(Y\)` at the average value of `\(X\)`, which is much more interpretable.

--

- Centering can be especially useful in models with interactions (which we are yet to explore).

--

- Centering can also help with multicollinearity (which we will dive into properly in the next class).

--

- Essentially, a transformed variable `\(x_j^2\)` may be highly correlated with the untransformed counterpart `\(x_j\)`, which we want to avoid. Centering `\(x_j\)` before taking the square helps with that.

--

- From now on for this example, we will mean center continuous predictors.


---
## Centering


```r
wages$agec &lt;- c(scale(wages$age,scale=F))
wages$seniorc &lt;- c(scale(wages$senior,scale=F))
wages$experc &lt;- c(scale(wages$exper,scale=F))
wages$educc &lt;- c(scale(wages$educ,scale=F))
regwagec &lt;- lm(bsal~ sex + seniorc + agec + educc + experc, data= wages)
summary(regwagec)
```

```
## 
## Call:
## lm(formula = bsal ~ sex + seniorc + agec + educc + experc, data = wages)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1217.36  -342.83   -55.61   297.10  1575.53 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 5924.0072    99.6588  59.443  &lt; 2e-16 ***
## sexFemale   -767.9127   128.9700  -5.954 5.39e-08 ***
## seniorc      -22.5823     5.2957  -4.264 5.08e-05 ***
## agec           0.6310     0.7207   0.876 0.383692    
## educc         92.3060    24.8635   3.713 0.000361 ***
## experc         0.5006     1.0553   0.474 0.636388    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 508.1 on 87 degrees of freedom
## Multiple R-squared:  0.5152,	Adjusted R-squared:  0.4873 
## F-statistic: 18.49 on 5 and 87 DF,  p-value: 1.811e-12
```


---
## Centering

- Notice that the coefficients for the predictors have not changed but the intercept has changed.

--

- We interpret the intercept as the average baseline salary for male employees who are 474 months old, have 82 months of seniority, 12.5 years of education, and 101 months of experience.
  
  ```r
  colMeans(wages[,c("age","senior","educ","exper")])
  ```
  
  ```
  ##       age    senior      educ     exper 
  ## 474.39785  82.27957  12.50538 100.92742
  ```

--

- Now, back to the model diagnostics and refinement.

--

- Let's add the squared terms of the centered age and centered experience predictors to the dataset and refit the model. 
  
  ```r
  wages$agec2 &lt;- wages$agec^2
  wages$experc2 &lt;- wages$experc^2
  ```


---
## Re-fitting the model


```r
regwagecsquares &lt;- lm(bsal~sex+seniorc+agec+agec2+educc+experc+experc2,data=wages)
summary(regwagecsquares)
```

```
## 
## Call:
## lm(formula = bsal ~ sex + seniorc + agec + agec2 + educc + experc + 
##     experc2, data = wages)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1086.84  -267.84    -8.71   304.92  1642.44 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.098e+03  1.123e+02  54.313  &lt; 2e-16 ***
## sexFemale   -7.684e+02  1.211e+02  -6.343 1.04e-08 ***
## seniorc     -1.764e+01  5.265e+00  -3.351  0.00120 ** 
## agec        -3.473e-01  7.814e-01  -0.444  0.65783    
## agec2        7.195e-04  4.045e-03   0.178  0.85925    
## educc        7.561e+01  2.406e+01   3.142  0.00231 ** 
## experc       4.035e+00  1.479e+00   2.729  0.00772 ** 
## experc2     -2.298e-02  7.592e-03  -3.027  0.00326 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 477 on 85 degrees of freedom
## Multiple R-squared:  0.5825,	Adjusted R-squared:  0.5481 
## F-statistic: 16.94 on 7 and 85 DF,  p-value: 8.011e-14
```


---
## Interpreting the new model

Clearly, .hlight[experc2] is an important predictor given all other predictors. However, it can be tough to interpret its effect using the coefficient. Instead, let's visualize the effect of changing experience.


```r
#First, make the 20 values of experience that you want to examine
newexper &lt;- seq(from=10,to=300,by=5)
newexperc &lt;- newexper - mean(wages$exper)
newexperc2 &lt;- newexperc^2
newdata &lt;- data.frame(matrix(0, nrow=length(newexper), ncol=7))
names(newdata) &lt;- c("sex","seniorc","agec","agec2","educc","experc","experc2")
newdata$experc &lt;- newexperc; newdata$experc2 &lt;- newexperc2; newdata$sex &lt;- "Male"
#Since we use mean-centered predictors, the rows in the new dataset correspond to 
#people with average values of seniority, age, and education. 
preds_male &lt;- predict(regwagecsquares,newdata,interval="confidence"); preds_male[1:3,]
```

```
##        fit      lwr      upr
## 1 5541.473 5114.091 5968.854
## 2 5581.974 5176.078 5987.871
## 3 5621.327 5235.858 6006.796
```

```r
newdata$sex &lt;- "Female"; 
preds_female &lt;- predict(regwagecsquares,newdata,interval="confidence"); preds_female[1:3,]
```

```
##        fit      lwr      upr
## 1 4773.033 4403.057 5143.009
## 2 4813.535 4466.389 5160.680
## 3 4852.887 4527.494 5178.281
```


---
## Interpreting the new model


```r
plot(y=preds_male[,"fit"],x=newexper,xlab="Experience",ylab="Predicted Wages",
     main="Expected Change in B.Wages with Experience",col="darkblue",ylim=c(4700,6300))
points(y=preds_female[,"fit"], x=newexper,col="orange")
legend("bottomright",c("Male","Female"),col=c("darkblue","orange"),lty=c(2,2))
#Remember that this is with average values of other predictors.
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Why do we have the exact same trend for male and female?
&lt;/div&gt;


---
## Interpreting the new model


```r
#if you want to get the 95% confidence bands on the plot as well, you can do the following
plot(y=preds_male[,"fit"], x=newexper, xlab="Experience", ylab="Predicted Wages",
     main="Expected Change in B.Wages with Experience",col="darkblue",ylim=c(4200,6700))
points(y=preds_female[,"fit"], x=newexper,col="orange")
legend("bottomright",c("Male","Female"),col=c("darkblue","orange"),lty=c(1,1))
lines(y=preds_male[,"lwr"],x=newexper,col="darkblue",lty=2)
lines(y=preds_male[,"upr"],x=newexper,col="darkblue",lty=2)
lines(y=preds_female[,"lwr"],x=newexper,col="orange",lty=2)
lines(y=preds_female[,"upr"],x=newexper,col="orange",lty=2)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;


---
## Leverage, influence, and standardized residuals

- The summer review material briefly covered outlier, high leverage and influential points.

--

- Individual observations can have large impact on the estimates of coefficients and SEs.

--

- Sometimes these points are obvious from scatter plots, and sometimes they are not, especially in multivariate data.

--

- Concepts and metrics of leverage, influence, and standardized residuals can help identify impactful and unusual points.

--

- An .hlight[outlier] is a data point whose value does not follow the general trend of the rest of the data.

--

- &lt;div class="question"&gt;
When does a data point have high leverage? When is a data point influential? How can we identify them?
&lt;/div&gt;


---
## Leverage

- Points with extreme predictor values are called .hlight[high leverage] points. 

--

- That is, the predictor values for these points are far outside the range of values for most of the other points.

--

- Thus, leverage has nothing to do with values of the response variable `\(\boldsymbol{y}\)`.

--

- Leverage points POTENTIALLY have large impact on the estimates of coefficients and SEs.

--

- For those very comfortable with linear algebra, the leverage score `\(h_{ii}\)` for record `\(i\)` is defined as the `\(i^\textrm{th}\)` diagonal element of the projection or hat matrix.
.block[
.small[
`$$\boldsymbol{H} = \boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T.$$`
]
]


---
## Leverage

- Recall that
.block[
.small[
`$$\hat{\boldsymbol{y}} = \left[\boldsymbol{X} \left(\boldsymbol{X}^T \boldsymbol{X}\right)^{-1} \boldsymbol{X}^T \right] \boldsymbol{y} = \boldsymbol{H}\boldsymbol{y}.$$`
]
]

--

- Therefore, the leverage score `\(h_{ii}\)` for observation `\(i\)` is a measure of its "influence" on the regression line.

--

- Some properties of `\(h_{ii}\)`:

  - `\(0 \leq h_{ii} \leq 1\)`.
  
  - `\(\mathbb{Var}[e_i] = (1-h_{ii}) \sigma^2\)`.
  
  - High leverage points are often determined by paying attention to any observation for which `\(h_{ii} &gt; 2p/n\)`.
  
  - Points with `\(h_{ii}\)` close to 1 will thus tend to have more of an impact on model fit.


---
## High leverage: what to do?

- Points with high leverage deserve special attention:
  
--

  - Make sure that they do not result from data entry errors.
  
--

  - Make sure that they are in scope for the types of individuals for which you want to make predictions.
  
--

  - Make sure that you look at the impact of those points on estimates, especially when you have interactions in the model.

--

- Just because a point is a high leverage point does not mean it will have a large effect on regression. When this happens, we say that the observation is .hlight[influential].

--

- Whether or not a high leverage point actually affects the regression line depends on the value of the response variable...


---
## Cook's distance

- What if a point has a large impact on the estimates of the regression coefficients?

--

  - Dropping that point should change the coefficients significantly.

--

  - Consequently, a significant change in the coefficients should also change that point's predicted `\(y_i\)` value by a lot.
  
--

- For every point, we could delete it, re-run the regression, and see which points lead to big changes in the predicted `\(y_i\)`'s; very time consuming!

--

- However, .hlight[Cook's distance] gives a formula for quantifying the influence of the `\(i^\textrm{th}\)` observation, if it is removed from the sample. We have
.block[
.small[
`$$D_i = \sum^n_{j=1} \dfrac{\left(\hat{y}_j - \hat{y}_{j(i)} \right)^2}{s_e^2(p+1)}$$`
]
]

  where `\(\hat{y}_{j(i)}\)` is the predicted value after excluding the `\(i^\textrm{th}\)` observation.


---
## Big Cook's distances: what to do?

- Examine Cook's distances to look for large values.

--

  - Make sure there are no data entry errors in those points.

--

  - For each point with high Cook's distance, fit the model with and without that point, and compare the results.
  
--

- The consensus seems to be that `\(D_i &gt;1\)` indicates an observation is an influential value, but we generally pay attention to observations with `\(D_i &gt;0.5\)`.

--

- If the results (predictions or scientific interpretations) do not change much, just report the final model based on all data points and you don't really need to report anything about the Cook's distances.

--

- If results change a lot, you have several options...


---
## Cook's distance: What to do if large changes in results?

- It is generally OK to drop observations based on PREDICTOR values if

--

  1. It is scientifically meaningful to do so; and
  
--
  
  2. You intended to fit a model over the smaller `\(X\)` range to begin with (and just forgot). When this is the case, you should mention this in your analysis write-up and be careful when making predictions to avoid extrapolation.

--

- It is generally NOT OK to drop an observation based on its RESPONSE value (assuming no data errors in that value). These are
legitimate observations and dropping them is essentially cheating by changing the data to fit the model.

--

- You should try transformations or collect more data.


---
## Standardized residuals (also called internally studentized residuals)

- How do we best identify outliers, i.e., points that don’t fit the pattern implied by the line? We look for points with relatively large residuals.

--

- It would be nice to have a common scale to interpret what a “big” residual is, across all problems.

--

- As with most metrics in statistics, we look at each residual divided by its standard error (hence the term standardized residual).

--

- The SE of any residual (that is, `\(e_i\)` and not `\(\epsilon_i\)`) depends on the values of the predictors.

--

- As such, it turns out that the residuals for high leverage predictors have smaller variance than residuals for low leverage predictors.

--

- Intuition: the regression line tries to fit high leverage points as closely as possible, which results in smaller residuals for those points.


---
## Standardized residuals (also called internally studentized residuals)

- Standardized residuals have a Normal(0,1) distribution.

--

- Values with large standardized residuals are outliers, in that they don’t fit the pattern implied by the line.

--

- Values with large standardized residuals are not necessarily influential on the regression line. A point can be an outlier without impacting the line. We need to examine their Cook's distance to determine influence.

--

- It turns out that the Cook's distance `\(D_i\)` can also be expressed using the leverage score `\(h_{ii}\)` and square of the internally Studentized residuals.

--

- In fact, really one should do residual plots with standardized residuals instead of regular ones, since they can reflect constant variance assumption when it is true.


---
## Back to our example

Can we try to identify (if any) outliers, high leverage points and influential points?


```r
plot(regwagecsquares,which=4)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Are there any high leverage points?
&lt;/div&gt;

---
## Standardized residuals: What to do if large outliers?


```r
plot(regwagecsquares,which=5)
```

&lt;img src="03-reg-model-assess_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

--
&lt;div class="question"&gt;
Are there any outliers or influential points?
&lt;/div&gt;


---
## Standardized residuals: What to do if large outliers?

- As before, it is generally OK to drop observations based on PREDICTOR values if

--

  1. It is scientifically meaningful to do so; and
  
--
  
  2. You intended to fit a model over the smaller `\(X\)` range to begin with (and just forgot). When this is the case, you should mention this in your analysis write-up and be careful when making predictions to avoid extrapolation.

--

- It is generally NOT OK to drop an observation based on its RESPONSE value (assuming no data errors in that value). These are
legitimate observations and dropping them is essentially cheating by changing the data to fit the model.

--

- You should try transformations or collect more data.

--

- Or just do nothing! It can be okay to have some outliers. Examine their influence on your results and report them.


---
## Mean squared error

- One particularly useful metric for measuring model fit (especially when the goal is prediction) is the mean squared error (MSE):
.block[
.small[
`$$\textrm{MSE} = \dfrac{1}{n} \sum_{i=1}^{n} \left(y_i - \hat{y}_i \right)^2.$$`
]
]

--

- This value will be small when our predictions `\(\hat{y}_i\)` are close to the true `\(y_i\)`'s. Some analysts and data scientists will often report the root mean squared error (RMSE) instead, which is simply the square root of MSE.

--

- While it may be useful to calculate .hlight[within-sample MSE] using the same dataset (usually referred to as .hlight[training data]) that was used to fit the model, it is often more useful to calculate .hlight[out-of-sample MSE] using a different dataset (usually referred to as  .hlight[test data]).

--

- In other words, while it may be great to know that our model fits the data used in fitting it well, it would be even better to see that our model also fits new or future data well.

--

- This is essentially asking the question: what does our model tell us about what might happen in the future?


---
## Mean squared error

- If we have a large amount of data, we can split our sample into training and test datasets.

--

- The test dataset should contain new observations `\((y_{1i},\boldsymbol{x}_{1i})\)` that are not represented in the training dataset `\((y_{0i},\boldsymbol{x}_{0i})\)`.

--

- Then the .hlight[test MSE] or .hlight[out-of-sample MSE] is given by
.block[
.small[
`$$\textrm{MSE}_\textrm{test} = \dfrac{1}{n_\textrm{test}} \sum_{i=1}^{n_\textrm{test}} \left(y_{1i} - \hat{y}_{1i} \right)^2.$$`
]
]

  where `\(\hat{y}_{1i}\)` is the predicted response for a new observation in the test dataset using the model developed in the training dataset, and `\(n_\textrm{test}\)` is the number of new observations in the test dataset. 

--

- The smaller the MSE (whether in-sample or out-of-sample), the better. 

--

- However, because "small" can be relative depending on the scale of `\(y\)`, we often use MSEs when comparing different models (again, particularly when the goal is prediction). We will see this in the next class.


---
## Training and test data

- Test data are important because of the problem of .hlight[overfitting].

--

- Overfitting arises when the model is working too hard to find the perfect predictions in the training data and is not broadly generalizable because it has been picking up some patterns that are just reflecting random error.

--

- We generally expect the test MSE to be somewhat larger than the training MSE because our model has been developed to minimize the training MSE.

--

- Overfitting refers to a situation in which a different model (generally a simpler one) fit to the training data would result in a smaller test MSE (indicating better out-of-sample prediction).

--

- We may be able to identify this problem when comparing the out-of-sample MSEs of different models (including the parsimonious models).

--

- Note that in small datasets, the random split of the data can have considerable impact on the results; out-of-sample MSEs can differ greatly depending on which random sample we take. 


---
## Training and test data

Let's explore this concept using the last fitted regression. We will use three different random splits. For the first split, we have

```r
#set the seed to ensure we can replicate the same result
set.seed(123)
train_index &lt;- sample(nrow(wages),round(0.7*nrow(wages)),replace=F)
train &lt;- wages[train_index,]
test &lt;- wages[-train_index,]
regwagecsquares_train &lt;- lm(bsal~sex+seniorc+agec+agec2+educc+experc+experc2,data=train)
y_test_pred &lt;- predict(regwagecsquares_train,test)
temp &lt;- cbind(test$bsal,y_test_pred);
colnames(temp) &lt;- c("Truth","Predicted"); temp[1:5,]
```

```
##    Truth Predicted
## 1   5040  5705.432
## 2   6300  6254.975
## 3   6000  6376.807
## 10  6900  6548.043
## 11  6900  6103.975
```

```r
testMSE &lt;- mean((test$bsal - y_test_pred)^2); testMSE
```

```
## [1] 273782.4
```

```r
sqrt(testMSE)
```

```
## [1] 523.2422
```


---
## Training and test data

For the second split, we have

```r
#now change the seed
set.seed(1234)
train_index &lt;- sample(nrow(wages),round(0.7*nrow(wages)),replace=F)
train &lt;- wages[train_index,]
test &lt;- wages[-train_index,]
regwagecsquares_train &lt;- lm(bsal~sex+seniorc+agec+agec2+educc+experc+experc2,data=train)
y_test_pred &lt;- predict(regwagecsquares_train,test)
temp &lt;- cbind(test$bsal,y_test_pred);
colnames(temp) &lt;- c("Truth","Predicted"); temp[1:5,]
```

```
##    Truth Predicted
## 1   5040  5705.884
## 7   8100  6390.656
## 11  6900  6136.419
## 12  5400  5795.275
## 13  6000  6385.950
```

```r
testMSE &lt;- mean((test$bsal - y_test_pred)^2); testMSE
```

```
## [1] 328104.3
```

```r
sqrt(testMSE)
```

```
## [1] 572.8039
```


---
## Training and test data

For the final split, we have

```r
#now change the seed
set.seed(12345)
train_index &lt;- sample(nrow(wages),round(0.7*nrow(wages)),replace=F)
train &lt;- wages[train_index,]
test &lt;- wages[-train_index,]
regwagecsquares_train &lt;- lm(bsal~sex+seniorc+agec+agec2+educc+experc+experc2,data=train)
y_test_pred &lt;- predict(regwagecsquares_train,test)
temp &lt;- cbind(test$bsal,y_test_pred);
colnames(temp) &lt;- c("Truth","Predicted"); temp[1:5,]
```

```
##    Truth Predicted
## 4   6000  5354.919
## 6   6840  5754.380
## 8   6000  5672.324
## 18  5280  4682.765
## 21  5400  5008.557
```

```r
testMSE &lt;- mean((test$bsal - y_test_pred)^2); testMSE
```

```
## [1] 199045.4
```

```r
sqrt(testMSE)
```

```
## [1] 446.145
```


---
## K-fold cross-validation

- This train/test method of model validation is often referred to as .hlight[cross-validation]. In general, one can use other metrics instead of just the MSE.

- .hlight[K-fold cross-validation] is a type of cross-validation that aims to address the issue of sensitivity of results to particular random data splits.

--

- Specifically, under `\(K\)`-fold cross-validation, split the data into `\(K\)` mutually-exclusive groups, called folds. 

--

- For the `\(k^{\textrm{th}}\)` fold, with `\(k=1,\ldots,K\)`, fit the model on all the remaining data excluding that `\(k^{\textrm{th}}\)` fold (that is, all the other folds combined) and use the `\(k^{\textrm{th}}\)` fold as the test or validation set.

--

- Repeat this `\(k\)` times, so that each fold has a turn as the validation set. Obtain the `\(\textrm{MSE}_\textrm{test}^{(k)}\)` for each `\(k\)`, and summarize the error using
.block[
.small[
`$$\textrm{Avg.MSE} = \dfrac{1}{K} \sum_{k=1}^{K} \textrm{MSE}_\textrm{test}^{(k)}.$$`
]
]


---
## Leave-one-out cross-validation

- A special case of .hlight[K-fold cross-validation] is the .hlight[Leave-one-out cross-validation], in which `\(K=n\)` (very computationally intensive except in special cases).

--

- Test error estimates using `\(k=5\)` or `\(k=10\)` have been shown to have good statistical properties, motivating these common choices.

--

- In the case of least squares, we can get an estimate of the average MSE from leave-one-out cross-validation using a simple formula (sadly, this does not hold in other models) based on the fit of only one model!

--

- The estimate is 
.block[
.small[
`$$\textrm{Avg.MSE} = \dfrac{1}{n} \sum_{i=1}^{n} \left(\dfrac{y_i - \hat{y}_i}{1-h_{ii}} \right)^2.$$`
]
]

  where `\(h_{ii}\)` is the leverage score of observation `\(i\)`. 
  
--

  &lt;div class="question"&gt;
How would high leverage points affect Avg.MSE in this case?
&lt;/div&gt;


---
## Final notes

- .block[
  Generally it is a good idea to start with exploratory data analysis (which we did a bit of in the last class) rather than jumping right into modeling.
  ]
  
--

- After fitting your model, model assessment and validation is A MUST! 

--

- In this class and outside of it, you should always assess and validate your models!

--

- We will look at other metrics for validating models later in the class when we get to other models. 

--

- For example, the MSE may not be the best metric to look at when dealing with binary outcomes. Or can it still be useful? We will see!

--

- In the next class, we will start to explore methods for model selection and including interaction effects in MLRs.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
