<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>MLR: interaction terms, transformations, multicollinearity and heteroscedasticity</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# MLR: interaction terms, transformations, multicollinearity and heteroscedasticity
### Dr. Olanrewaju Michael Akande
### Sept 10, 2019

---







## Housekeeping

- Homework 2 is now online. Due date is 10:05am next Tuesday, September 17.

- Submit on Gradescope on or before 10:05am!


## Outline

- Questions from last lecture

- Special predictors: higher order terms, indicator (dummy) variables and interaction terms

- Transformations

- Multicollinearity

- Heteroscedasticity

- Final notes


---
## Introduction

- In the last class, we covered model assessment and validation. We also talked about the concept of cross validation.

--

- Today, we will talk about modeling and interpreting special predictors in regression models.

--

- Specifically, we will talk about higher order terms, indicator (dummy) variables and interaction terms. We have already talked about the first two, so we will simply formalize the concepts.

--

- This is lead us to start to talk about model selection.

--

- In the next class, we will start to talk about building, selecting and comparing MLR models. 


---
class: center, middle

# Special predictors



---
## Special predictors: higher order terms

- From the Harris Trust and Savings Bank example, we have already seen that the relationships between a response variable and some of the predictors can be potentially nonlinear. 

--

- Sometimes our outcome of interest can appear to have quadratic or even higher order polynomial trends with some predictors.

--

- In that particular example, we included squared terms for both age and experience.

--

- Whenever this is the case, we should look to include squared terms or higher order powers for predictors to capture trends.

--

- .block[General practice: include all lower order terms when including higher order ones (even if the lower order terms are not significant). This aids interpretation.
]

--

- As we have seen before, the best way to present result when including quadratic/polynomial trends is to plot the predicted average of `\(Y\)` for different values of `\(X\)`.



---
## Special predictors: indicator/dummy variables

- From the Harris Trust and Savings Bank, we have also seen how to include binary variables in a MLR model with the variable `sex`.

--

- In the example, we could actually have used the variable `fsex` (where 1=female and 0=male) instead of `sex` to give us the same exact results.

--

- That means that we also could have made a variable equal to one for all males and zero for all females, instead.

--

- The value of that coefficient would be `\(767\)` instead of `\(-767\)` like we had. All other statistics stay the same (SE, t-stat, p-value). Other coefficients also remain the same.

--

- &lt;div class="question"&gt;
In that example, is it possible to include both the male and female indicator variables in the same model? Why or why not?
&lt;/div&gt;

--

- Turns out that we cannot include indicator variables for the two values of the same binary variable when we also include the intercept.



---
## Special predictors: indicator/dummy variables

- It is not possible to estimate all three of these parameters in the same model uniquely.

--

- The exact same problem arises for any set of predictors such that one is an exact linear combination of the others.

--

- Example: Consider a regression model with dummy variables for both males and females, plus an intercept.
.block[
.small[
`$$y_i = \beta_0 + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i = \beta_0*1 + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i$$`
]
]

--

- Note that `\(\textrm{M}_i + \textrm{F}_i = 1\)` for all cases. Thus,
.block[
.small[
`$$y_i = \beta_0*(\textrm{M}_i + \textrm{F}_i) + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i = (\beta_0+\beta_1) \textrm{M}_i + (\beta_0+\beta_2) \textrm{F}_i + \epsilon_i.$$`
]
]

  We can estimate `\((\beta_0+\beta_1)\)` and `\((\beta_0+\beta_2)\)` but not all three uniquely.

--

- .block[Side note: there is no need to mean center dummy variables, since they have a natural interpretation at zero.]



---
## Special predictors: indicator/dummy variables

- What if a categorical variable has `\(k &gt; 2\)` levels?

--

- Make `\(k\)` dummy variables, one for each level. 

--

- Use only `\(k-1\)` of the levels in the regression model, since we cannot uniquely estimate all `\(k\)` at once if we also include an intercept (see previous slide).

--

- Excluded level is called the baseline.

--

- Most statistical softwares actually do this for you; that is, make the `\(k-1\)` dummy variables and set the first level as the baseline automatically.

--

- Values of coefficients of dummy variables are interpreted as changes in average `\(Y\)` over the baseline.

--

- We will go through in-class examples later today.



---
## Special predictors: interaction terms

- Sometimes relationship of some predictor with `\(Y\)` depends on values of other predictors.

--

- This is called an .hlight[interaction effect].

--

- An example of interaction effect for the Harris Bank dataset would be if the effect of age on baseline income was different for male versus female.

--

- That is, what if older males are paid more starting salaries than younger males but the reverse is actually the case for females?

--

- How do we account for such interaction effects? Make an interaction predictor: multiply one predictor times the other predictor.

--

- .block[General practice to include all main effects (each variable without interaction) when including interactions.]

--

- Again, we will go through in-class examples later today.



---
## Testing if groups of coefficients are equal to zero

- With so many variables (polynomial terms, dummy variables and interactions) in a linear model, we may want to test if multiple coefficients are equal to zero or not.

--

- We can do so using an F test (also called nested F test in this case).

--

- First suppose we fit a MLR model with all `\(p\)` predictors. We can compute the sum of squares of the errors (SSE) or residual sum of squares (RSS) for the model, that is,
.block[
.small[
`$$\textrm{RSS} = \sum^n_{i=1} \left(y_i - \hat{y}_i \right)^2.$$`
]
]

--

- Now to test that a particular subset of `\(q\)` of the coefficients are zero.
.block[
.small[
`$$H_0: \beta_{p-q+1} = \beta_{p-q+2} = \ldots = \beta_p = 0.$$`
]
]

--

- Let's call the residual sum of squares for that model `\(\textrm{RSS}_0\)`.


---
## Testing if groups of coefficients are equal to zero

- Then the appropriate F-statistic is
.block[
.small[
`$$F = \dfrac{(\textrm{RSS}_0 - \textrm{RSS})/q}{\textrm{RSS}/(n-p-1)}.$$`
]
]

--

- To calculate the p-value, look for the area under the `\(F\)` curve with `\(q\)` degrees of freedom in the numerator, and `\((n-p-1)\)` degrees of freedom in the denominator.

--

- Guess what? As is the case with pretty much everything else we do in this class, this is so easy to do in R!

--

&lt;img src="img/surprised-baby.jpeg" width="250px" height="200px" style="display: block; margin: auto;" /&gt;



---
## The problem of multicollinearity

- Just like we had with the dummy variables, you cannot include two variables with a perfect linear association as predictors in regression.

--

- Example: suppose the true population line is
.block[
.small[
`$$\textrm{Avg. y} = 3 + 4x.$$`
]
]

--

- Suppose we try to include `\(x\)` and `\(z = x/10\)` as predictors in our own model,
- Example: suppose the true population line is
.block[
.small[
`$$\textrm{Avg. y} = \beta_0 + \beta_1 x + \beta_2 z,$$`
]
]

  and estimate all coefficients. Since `\(z = x/10\)`, we have
.block[
.small[
`$$\textrm{Avg. y} = \beta_0 + \beta_1 x + \beta_2 \dfrac{x}{10} = \beta_0 + \left( \beta_1 +  \dfrac{\beta_2}{10} x \right)$$`
]
]

--

- We could set `\(\beta_1\)` and `\(\beta_2\)` to ANY two numbers such that `\(\beta_1 +  \beta_2/10 = 4\)`. The data cannot pick from the possible combinations.



---
## The problem of multicollinearity

- In real data, when we get “close” to perfect colinearities we see standard errors inflate, sometimes massively.

--

- When might we get close:
  + Very high correlations ($&gt; 0.9$) among two (or more) predictors in modest sample sizes
  + When one or more variables are nearly a linear combination of the others.
  + Including quadratic terms as predictors without first mean centering the values before squaring.
  + Including interactions involving continuous variables.
  
  
---
## The problem of multicollinearity

- How to diagnose:
  + Look at a correlation matrix of all the predictors (including dummy variables). Look for values near 1 or 1.
  + If you are suspicious that some predictor is a near linear combination of others, run a regression of that predictor on all other predictors (not including Y) to see if R squared is near 1.
  
      If the R squared is near 1, you should think about centering your variables or maybe even excluding that variable from your regression in some cases.


---
## Final notes

-
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
