---
title: "MLR: interaction terms, transformations, multicollinearity and heteroscedasticity"
author: "Dr. Olanrewaju Michael Akande"
date: "Sept 10, 2019"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
#library(tidyverse)
#library(magick)
library(knitr)
library(kableExtra)
library(lattice)
#library(dplyr)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  #show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 2.65,dpi =300,fig.align='center',fig.show='hold',size='footnotesize',small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}

knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar =  c(4, 4, 1.5, 1.5)) 
})
```


## Outline

- Recap of last lecture

- Special predictors: higher order terms

- Special predictors: indicator (dummy) variables

- Special predictors: interaction terms

- Transformations

- Multicollinearity

- Heteroscedasticity

- Final notes


---
## Introduction

- In the last class, we covered model assessment and validation. We also talked about the concept of cross validation.

--

- Today, we will talk about modeling and interpreting special predictors in regression models.

--

- Specifically, we will talk about higher order terms, indicator (dummy) variables and interaction terms. We have already talked about the first two, so we will simply formalize the concepts.

--

- This is lead us to start to talk about model selection.

--

- In the next class, we will start to talk about building, selecting and comparing MLR models. 


---
class: center, middle

# Special predictors



---
## Special predictors: higher order terms

- From the Harris Trust and Savings Bank example, we have already seen that the relationships between a response variable and some of the predictors can be potentially nonlinear. 

--

- Sometimes our outcome of interest can appear to have quadratic or even higher order polynomial trends with some predictors.

--

- In that particular example, we included squared terms for both age and experience.

--

- Whenever this is the case, we should look to include squared terms or higher order powers for predictors to capture trends.

--

- .block[General practice: include all lower order terms when including higher order ones (even if the lower order terms are not significant). This aids interpretation.
]

--

- As we have seen before, the best way to present result when including quadratic/polynomial trends is to plot the predicted average of $Y$ for different values of $X$.



---
## Special predictors: indicator/dummy variables

- From the Harris Trust and Savings Bank, we have also seen how to include binary variables in a MLR model with the variable `sex`.

--

- In the example, we could actually have used the variable `fsex` (where 1=female and 0=male) instead of `sex` to give us the same exact results.

--

- That means that we also could have made a variable equal to one for all males and zero for all females, instead.

--

- The value of that coefficient would be $767$ instead of $-767$ like we had. All other statistics stay the same (SE, t-stat, p-value). Other coefficients also remain the same.

--

- <div class="question">
In that example, is it possible to include both the male and female indicator variables in the same model? Why or why not?
</div>

--

- Turns out that we cannot include indicator variables for the two values of the same binary variable when we also include the intercept.



---
## Special predictors: indicator/dummy variables

- It is not possible to estimate all three of these parameters in the same model uniquely.

--

- The exact same problem arises for any set of predictors such that one is an exact linear combination of the others.

--

- Example: Consider a regression model with dummy variables for both males and females, plus an intercept.
.block[
.small[
$$y_i = \beta_0 + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i = \beta_0*1 + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i$$
]
]

--

- Note that $\textrm{M}_i + \textrm{F}_i = 1$ for all cases. Thus,
.block[
.small[
$$y_i = \beta_0*(\textrm{M}_i + \textrm{F}_i) + \beta_1 \textrm{M}_i + \beta_2 \textrm{F}_i + \epsilon_i = (\beta_0+\beta_1) \textrm{M}_i + (\beta_0+\beta_2) \textrm{F}_i + \epsilon_i.$$
]
]

  We can estimate $(\beta_0+\beta_1)$ and $(\beta_0+\beta_2)$ but not all three uniquely.

--

- .block[Side note: there is no need to mean center dummy variables, since they have a natural interpretation at zero.]



---
## Special predictors: indicator/dummy variables

- What if a categorical variable has $k > 2$ levels?

--

- Make $k$ dummy variables, one for each level. 

--

- Use only $k-1$ of the levels in the regression model, since we cannot uniquely estimate all $k$ at once if we also include an intercept (see previous slide).

--

- Excluded level is called the baseline.

--

- Most statistical softwares actually do this for you; that is, make the $k-1$ dummy variables and set the first level as the baseline automatically.

--

- Values of coefficients of dummy variables are interpreted as changes in average $Y$ over the baseline.

--

- We will go through in-class examples later today.



---
## Special predictors: interaction terms

- Sometimes relationship of some predictor with $Y$ depends on values of other predictors.

--

- This is called an .hlight[interaction effect].

--

- An example of interaction effect for the Harris Bank dataset would be if the effect of age on baseline income was different for male versus female.

--

- That is, what if older males are paid more starting salaries than younger males but the reverse is actually the case for females?

--

- How do we account for such interaction effects? Make an interaction predictor: multiply one predictor times the other predictor.

--

- .block[General practice to include all main effects (each variable without interaction) when including interactions.]

--

- Again, we will go through in-class examples later today.



---
## Testing if groups of coefficients are equal to zero

- With so many variables (polynomial terms, dummy variables and interactions) in a linear model, we may want to test if multiple coefficients are equal to zero or not.

--

- We can do so using an F test (also called nested F test in this case).

--

- First suppose we fit a MLR model with all $p$ predictors. We can compute the residual sum of squares (RSS) for the model, that is,
.block[
.small[
$$\textrm{RSS} = \sum^n_{i=1} \left(y_i - \hat{y}_i \right)^2.$$
]
]

--

- Now to test that a particular subset of $q$ of the coefficients are zero.
.block[
.small[
$$H_0: \beta_{p-q+1} = \beta_{p-q+2} = \ldots = \beta_p = 0.$$
]
]

--

- Let's call the residual sum of squares for that model $\textrm{RSS}_0$.


---
## Testing if groups of coefficients are equal to zero

- Then the appropriate F-statistic is
.block[
.small[
$$F = \dfrac{(\textrm{RSS}_0 - \textrm{RSS})/q}{\textrm{RSS}/(n-p-1)}.$$
]
]

--

- To calculate the p-value, look for the area under the $F$ curve with $q$ degrees of freedom in the numerator, and $(n-p-1)$ degrees of freedom in the denominator.

--

- Guess what? This is so easy to do in R!

--

```{r echo=FALSE, out.width="250px", out.height="200px"}
knitr::include_graphics("img/surprised-baby.jpeg")
```



---
## The problem of multicollinearity

- Just like we had with the dummy variables, you cannot include two variables with a perfect linear association as predictors in regression.

--

- Example: suppose the true population line is
.block[
.small[
$$\textrm{Avg. y} = 3 + 4x.$$
]
]

--

- Suppose we try to include $x$ and $z = x/10$ as predictors in our own model,
- Example: suppose the true population line is
.block[
.small[
$$\textrm{Avg. y} = \beta_0 + \beta_1 x + \beta_2 z,$$
]
]

  and estimate all coefficients. Since $z = x/10$, we have
.block[
.small[
$$\textrm{Avg. y} = \beta_0 + \beta_1 x + \beta_2 \dfrac{x}{10} = \beta_0 + \left( \beta_1 +  \dfrac{\beta_2}{10} x \right)$$
]
]

--

- We could set $\beta_1$ and $\beta_2$ to ANY two numbers such that $\beta_1 +  \beta_2/10 = 4$. The data cannot pick from the possible combinations.



---
## The problem of multicollinearity

- In real data, when we get “close” to perfect colinearities we see standard errors inflate, sometimes massively.

--

- When might we get close:
  + Very high correlations ($> 0.9$) among two (or more) predictors in modest sample sizes
  + When one or more variables are nearly a linear combination of the others.
  + Including quadratic terms as predictors without first mean centering the values before squaring.
  + Including interactions involving continuous variables.
  
  
---
## The problem of multicollinearity

- How to diagnose:
  + Look at a correlation matrix of all the predictors (including dummy variables). Look for values near 1 or 1.
  + If you are suspicious that some predictor is a near linear combination of others, run a regression of that predictor on all other predictors (not including Y) to see if R squared is near 1.
  
      If the R squared is near 1, you should think about centering your variables or maybe even excluding that variable from your regression in some cases.


---
## Final notes

- 

